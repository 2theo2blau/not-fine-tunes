{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from bitsandbytes.optim import AdamW8bit\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"mistralai/Mistral-Nemo-Base-2407\"\n",
    "# --- Dataset Configuration ---\n",
    "DATA_DIR = \"nemo-rawtext-data\"  # Directory containing .txt files\n",
    "TEST_SPLIT_SIZE = 0.1  # 10% of the data will be used for validation\n",
    "BLOCK_SIZE = 1024      # Context window size for the model\n",
    "\n",
    "# --- Training Configuration ---\n",
    "OUTPUT_DIR = \"mistral-nemo-continued-pretrain-accelerate\"\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "LEARNING_RATE = 2e-4\n",
    "BATCH_SIZE = 4        # This will be the per-device batch size\n",
    "GRAD_ACCUM_STEPS = 4  # Gradient accumulation steps\n",
    "\n",
    "# --- Checkpoint Management ---\n",
    "EVAL_STEPS = 50       # Evaluate every N training steps\n",
    "SAVE_TOTAL_LIMIT = 3  # Keep only the best 3 checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(gradient_accumulation_steps=GRAD_ACCUM_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all .txt files recursively in the specified directory\n",
    "text_files = glob.glob(f'{DATA_DIR}/**/*.txt', recursive=True)\n",
    "if not text_files:\n",
    "    raise ValueError(f\"No .txt files found in {DATA_DIR}. Please check the path.\")\n",
    "\n",
    "# Load the text files into a dataset\n",
    "raw_datasets = load_dataset('text', data_files=text_files, split='train')\n",
    "\n",
    "# Create a train/test split\n",
    "split_datasets = raw_datasets.train_test_split(test_size=TEST_SPLIT_SIZE, shuffle=True, seed=42)\n",
    "train_dataset = split_datasets['train']\n",
    "eval_dataset = split_datasets['test']\n",
    "\n",
    "accelerator.print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "accelerator.print(f\"Validation dataset size: {len(eval_dataset)}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenizing and grouping functions (same as before)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    if total_length >= BLOCK_SIZE:\n",
    "        total_length = (total_length // BLOCK_SIZE) * BLOCK_SIZE\n",
    "    result = {\n",
    "        k: [t[i : i + BLOCK_SIZE] for i in range(0, total_length, BLOCK_SIZE)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    # Apply processing on the main process first to prevent race conditions\n",
    "    tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=list(train_dataset.features))\n",
    "    lm_train_dataset = tokenized_train.map(group_texts, batched=True)\n",
    "\n",
    "    tokenized_eval = eval_dataset.map(tokenize_function, batched=True, remove_columns=list(eval_dataset.features))\n",
    "    lm_eval_dataset = tokenized_eval.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load base model - IMPORTANT: we do not use device_map=\"auto\" here.\n",
    "# Accelerate will handle the device placement.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "# PEFT/LoRA configuration\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Collator and DataLoaders\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "train_dataloader = DataLoader(lm_train_dataset, shuffle=True, collate_fn=data_collator, batch_size=BATCH_SIZE)\n",
    "eval_dataloader = DataLoader(lm_eval_dataset, collate_fn=data_collator, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Optimizer and Scheduler\n",
    "optimizer = AdamW8bit(peft_model.parameters(), lr=LEARNING_RATE)\n",
    "num_update_steps_per_epoch = len(train_dataloader) // GRAD_ACCUM_STEPS\n",
    "total_training_steps = NUM_TRAIN_EPOCHS * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_training_steps,\n",
    ")\n",
    "\n",
    "# Prepare everything with Accelerate\n",
    "peft_model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    peft_model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_bar = tqdm(range(total_training_steps), disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "best_checkpoints = [] # List to store (validation_loss, path)\n",
    "\n",
    "for epoch in range(NUM_TRAIN_EPOCHS):\n",
    "    peft_model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        with accelerator.accumulate(peft_model):\n",
    "            outputs = peft_model(**batch)\n",
    "            loss = outputs.loss\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if accelerator.sync_gradients:\n",
    "            progress_bar.update(1)\n",
    "            completed_steps += 1\n",
    "\n",
    "        # --- Evaluation and Checkpointing Logic ---\n",
    "        if completed_steps > 0 and completed_steps % EVAL_STEPS == 0:\n",
    "            peft_model.eval()\n",
    "            losses = []\n",
    "            for eval_step, eval_batch in enumerate(eval_dataloader):\n",
    "                with torch.no_grad():\n",
    "                    outputs = peft_model(**eval_batch)\n",
    "                loss = outputs.loss\n",
    "                losses.append(accelerator.gather_for_metrics(loss.repeat(BATCH_SIZE)))\n",
    "\n",
    "            losses = torch.cat(losses)\n",
    "            try:\n",
    "                eval_loss = torch.mean(losses)\n",
    "                perplexity = torch.exp(eval_loss)\n",
    "            except OverflowError:\n",
    "                perplexity = float(\"inf\")\n",
    "\n",
    "            accelerator.print(f\"Step {completed_steps}: Eval Loss: {eval_loss:.4f}, Perplexity: {perplexity:.4f}\")\n",
    "\n",
    "            # --- Save and Manage Checkpoints on Main Process ---\n",
    "            if accelerator.is_main_process:\n",
    "                checkpoint_dir = os.path.join(OUTPUT_DIR, f\"checkpoint-{completed_steps}\")\n",
    "                accelerator.save_state(checkpoint_dir)\n",
    "                \n",
    "                # Add checkpoint to our list\n",
    "                best_checkpoints.append((eval_loss, checkpoint_dir))\n",
    "                # Sort checkpoints by loss (best first)\n",
    "                best_checkpoints.sort(key=lambda x: x[0])\n",
    "\n",
    "                # If we have more checkpoints than the limit, remove the worst one\n",
    "                if len(best_checkpoints) > SAVE_TOTAL_LIMIT:\n",
    "                    worst_checkpoint_loss, worst_checkpoint_dir = best_checkpoints.pop()\n",
    "                    accelerator.print(f\"Removing old checkpoint with loss {worst_checkpoint_loss:.4f}: {worst_checkpoint_dir}\")\n",
    "                    if os.path.exists(worst_checkpoint_dir):\n",
    "                        shutil.rmtree(worst_checkpoint_dir)\n",
    "\n",
    "            peft_model.train() # Switch back to training mode\n",
    "\n",
    "        if completed_steps >= total_training_steps:\n",
    "            break\n",
    "    if completed_steps >= total_training_steps:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if accelerator.is_main_process:\n",
    "    final_adapter_dir = os.path.join(OUTPUT_DIR, \"final_adapter\")\n",
    "    os.makedirs(final_adapter_dir, exist_ok=True)\n",
    "    \n",
    "    # To save the PEFT adapter, we need the unwrapped model\n",
    "    unwrapped_model = accelerator.unwrap_model(peft_model)\n",
    "    unwrapped_model.save_pretrained(final_adapter_dir)\n",
    "    \n",
    "    accelerator.print(f\"Final PEFT adapter saved to {final_adapter_dir}\")\n",
    "    accelerator.print(\"Training complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetunes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
