{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 02-16 22:46:26 __init__.py:190] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.2.5: Fast Mistral patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.57 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = torch.bfloat16\n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"mistralai/Mistral-7B-v0.3\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading input_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theo/miniconda3/envs/finetunes/lib/python3.12/site-packages/unsloth/models/_utils.py:752: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  offloaded_W = torch.load(filename, map_location = \"cpu\", mmap = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Training embed_tokens in mixed precision to save VRAM\n",
      "Unsloth: Training lm_head in mixed precision to save VRAM\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"o_proj\", \"k_proj\",\n",
    "                    \"gate_proj\", \"down_proj\", \"up_proj\", \n",
    "                    \"embed_tokens\",\"lm_head\"],\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=True,\n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a974d697b84e44a5dd529cd01c8656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "def load_jsonl_dataset(jsonl_path: str) -> Dataset:\n",
    "    data_list = []\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                data_list.append(json.loads(line))\n",
    "    return Dataset.from_list(data_list)\n",
    "\n",
    "dataset = load_jsonl_dataset(\"./processed_data/rawtext/theowriting.jsonl\")\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    return { \"text\" : [example + EOS_TOKEN for example in examples[\"text\"]] }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "I think of all the nicknames I‚Äôve received or given to myself, I like ‚Äúse√±or de pastry‚Äù the most.\n",
      "\n",
      "Frankly, even pastry aside, I think it‚Äôs an apt descriptor of where I sit socially. It also implies something of a disconnect or unease with ‚Äúpastry‚Äù on the whole. Take that to mean what you will.\n",
      "\n",
      "It‚Äôs not even a real barrier. It‚Äôs just the ‚Äúse√±or‚Äù of it all. That makes me feel like an odd one out.\n",
      "\n",
      "Because I love pastry, and I love what it stands for ‚Äî in my mind, at least. It means warmth and welcoming. It means bluntness and no nonsense. And most of all, it means care and regard for the little things.\n",
      "\n",
      "I don‚Äôt even particularly love sweets the way I once did. Not consuming them, at least. But I love constructing them, and I love what they mean to others. I also, somewhat selfishly, love what they have done for me. \n",
      "\n",
      "I could wax on, like many others have before me, about the universality of food. About its appeal to the caregiving nature of humans, and its incredible power to transcend language and culture. But I‚Äôm not going to do that. I‚Äôm going to discuss how pastries can buy good will. \n",
      "\n",
      "You may be familiar with what I see as the other side of this. If you‚Äôre in a profession whose services are even vaguely tangential to regular folks‚Äô day-to-day ‚Äî whether that be cooking, IT, or web design ‚Äî you‚Äôve undoubtedly gotten an offhand ‚Äúwhen are you going to make me a cake/fix my WiFi/build me a website?‚Äù from friends or family. Almost always with the implication that it will be done for free, and that doing so is asking nothing of you. That when you do these things for a living, that means it can be done effortlessly. If you‚Äôre anything like me, you don‚Äôt like this.\n",
      "\n",
      "But on the other hand is the unsolicited provision of these services to people who appreciate the effort and thought that goes into these things. This is what I am referring to when I say ‚Äúbuying good will.‚Äù That is probably an over-simplistic way of looking at it; reductive, some might say. There‚Äôs an emotional component as well. But, few things are so universally received as an act of affectation as presenting someone with a baked good. That‚Äôs not to say that \n",
      "\n",
      "All of this is to say that to me, pastry represents a kind of unspoken affection that is very special. I would never do this intentionally, but somehow who the thing is being made for is always made evident in the final output. At least, it is if you look closely enough.\n",
      "\n",
      "I don‚Äôt even mean this in a romantic sense. Purely in the sense of care and admiration. These things inevitably shine through.</s>\n",
      "=========================\n",
      "Something important about the path that I‚Äôve chosen is that while it may be the ‚Äúharder‚Äù way to do things, it isn‚Äôt harder for me. Sure, it may entail more rejection. It may mean I won‚Äôt fall into the kind of prestige that I‚Äôm not sure I actually want. It almost certainly means that.\n",
      "\n",
      "But it works for me. It feels eminently doable. Just more work than the alternative. But in many ways, it‚Äôs not. Because it‚Äôs simultaneously conducive to the kind of personal development that I want, without that running counter to my career. It‚Äôs giving me the space I need to be honest with myself about the things I‚Äôve never told anyone. It makes me want to do things in the way that, long-term, will leave me far better off than forcing myself to finish a degree that was making me feel like I was being rended in two.\n",
      "\n",
      "I know what boundaries I‚Äôd like to draw for myself in the short term. I know that these boundaries are a stepping stone to what I really want. That it gets harder before it gets easier. That I do not want to get back on the path I was going down maybe 4 months ago.\n",
      "\n",
      "That may seem hypocritical with additional context. It is. \n",
      "\n",
      "I also know that for how proud I am of pastry, why it‚Äôs not enough.</s>\n"
     ]
    }
   ],
   "source": [
    "for row in dataset[:2][\"text\"]:\n",
    "    print(\"=========================\")\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3134616636.py, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 33\u001b[0;36m\u001b[0m\n\u001b[0;31m    )\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bf16_supported\n",
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "\n",
    "trainer = UnslothTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=8,\n",
    "\n",
    "    args=UnslothTrainingArguments(\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "\n",
    "        warmup_ratio=0.05,\n",
    "        num_train_epochs=10,\n",
    "\n",
    "        learning_rate=1e-5,\n",
    "        embedding_learning_rate=1e-5,\n",
    "\n",
    "        fp16 = not is_bf16_supported(),\n",
    "        bf16 = is_bf16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_torch\",\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        weight_decay=0.01,\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetunes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
